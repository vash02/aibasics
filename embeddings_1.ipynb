{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-10T18:11:11.610854Z",
     "start_time": "2025-03-10T18:11:05.012775Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\r\n",
      "  Obtaining dependency information for gensim from https://files.pythonhosted.org/packages/e4/0d/d60f023abd74e1ccd448c97ec9c0d78ddc43a95497c14939a05c5de6f887/gensim-4.3.3-cp39-cp39-macosx_11_0_arm64.whl.metadata\r\n",
      "  Using cached gensim-4.3.3-cp39-cp39-macosx_11_0_arm64.whl.metadata (8.3 kB)\r\n",
      "Collecting nltk\r\n",
      "  Obtaining dependency information for nltk from https://files.pythonhosted.org/packages/4d/66/7d9e26593edda06e8cb531874633f7c2372279c3b0f46235539fe546df8b/nltk-3.9.1-py3-none-any.whl.metadata\r\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\r\n",
      "Collecting numpy<2.0,>=1.18.5 (from gensim)\r\n",
      "  Obtaining dependency information for numpy<2.0,>=1.18.5 from https://files.pythonhosted.org/packages/ae/8c/ab03a7c25741f9ebc92684a20125fbc9fc1b8e1e700beb9197d750fdff88/numpy-1.26.4-cp39-cp39-macosx_11_0_arm64.whl.metadata\r\n",
      "  Using cached numpy-1.26.4-cp39-cp39-macosx_11_0_arm64.whl.metadata (61 kB)\r\n",
      "Collecting scipy<1.14.0,>=1.7.0 (from gensim)\r\n",
      "  Obtaining dependency information for scipy<1.14.0,>=1.7.0 from https://files.pythonhosted.org/packages/5c/c0/e71b94b20ccf9effb38d7147c0064c08c622309fd487b1b677771a97d18c/scipy-1.13.1-cp39-cp39-macosx_12_0_arm64.whl.metadata\r\n",
      "  Using cached scipy-1.13.1-cp39-cp39-macosx_12_0_arm64.whl.metadata (60 kB)\r\n",
      "Collecting smart-open>=1.8.1 (from gensim)\r\n",
      "  Obtaining dependency information for smart-open>=1.8.1 from https://files.pythonhosted.org/packages/7a/18/9a8d9f01957aa1f8bbc5676d54c2e33102d247e146c1a3679d3bd5cc2e3a/smart_open-7.1.0-py3-none-any.whl.metadata\r\n",
      "  Using cached smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\r\n",
      "Collecting click (from nltk)\r\n",
      "  Obtaining dependency information for click from https://files.pythonhosted.org/packages/7e/d4/7ebdbd03970677812aac39c869717059dbb71a4cfc033ca6e5221787892c/click-8.1.8-py3-none-any.whl.metadata\r\n",
      "  Using cached click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\r\n",
      "Collecting joblib (from nltk)\r\n",
      "  Obtaining dependency information for joblib from https://files.pythonhosted.org/packages/91/29/df4b9b42f2be0b623cbd5e2140cafcaa2bef0759a00b7b70104dcfe2fb51/joblib-1.4.2-py3-none-any.whl.metadata\r\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\r\n",
      "Collecting regex>=2021.8.3 (from nltk)\r\n",
      "  Obtaining dependency information for regex>=2021.8.3 from https://files.pythonhosted.org/packages/7a/d1/598de10b17fdafc452d11f7dada11c3be4e379a8671393e4e3da3c4070df/regex-2024.11.6-cp39-cp39-macosx_11_0_arm64.whl.metadata\r\n",
      "  Using cached regex-2024.11.6-cp39-cp39-macosx_11_0_arm64.whl.metadata (40 kB)\r\n",
      "Collecting tqdm (from nltk)\r\n",
      "  Obtaining dependency information for tqdm from https://files.pythonhosted.org/packages/d0/30/dc54f88dd4a2b5dc8a0279bdd7270e735851848b762aeb1c1184ed1f6b14/tqdm-4.67.1-py3-none-any.whl.metadata\r\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\r\n",
      "Collecting wrapt (from smart-open>=1.8.1->gensim)\r\n",
      "  Obtaining dependency information for wrapt from https://files.pythonhosted.org/packages/fa/9b/e172c8f28a489a2888df18f953e2f6cb8d33b1a2e78c9dfc52d8bf6a5ead/wrapt-1.17.2-cp39-cp39-macosx_11_0_arm64.whl.metadata\r\n",
      "  Using cached wrapt-1.17.2-cp39-cp39-macosx_11_0_arm64.whl.metadata (6.4 kB)\r\n",
      "Using cached gensim-4.3.3-cp39-cp39-macosx_11_0_arm64.whl (24.0 MB)\r\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.5/1.5 MB\u001B[0m \u001B[31m3.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hUsing cached numpy-1.26.4-cp39-cp39-macosx_11_0_arm64.whl (14.0 MB)\r\n",
      "Using cached regex-2024.11.6-cp39-cp39-macosx_11_0_arm64.whl (284 kB)\r\n",
      "Using cached scipy-1.13.1-cp39-cp39-macosx_12_0_arm64.whl (30.3 MB)\r\n",
      "Using cached smart_open-7.1.0-py3-none-any.whl (61 kB)\r\n",
      "Using cached click-8.1.8-py3-none-any.whl (98 kB)\r\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\r\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\r\n",
      "Using cached wrapt-1.17.2-cp39-cp39-macosx_11_0_arm64.whl (38 kB)\r\n",
      "Installing collected packages: wrapt, tqdm, regex, numpy, joblib, click, smart-open, scipy, nltk, gensim\r\n",
      "Successfully installed click-8.1.8 gensim-4.3.3 joblib-1.4.2 nltk-3.9.1 numpy-1.26.4 regex-2024.11.6 scipy-1.13.1 smart-open-7.1.0 tqdm-4.67.1 wrapt-1.17.2\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.0.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vaibhav/PycharmProjects/aiBasics/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "[nltk_data] Downloading package punkt to /Users/vaibhav/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package reuters to /Users/vaibhav/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample sentence: ['asian', 'exporters', 'fear', 'damage', 'from', 'rift', 'mounting', 'trade', 'friction', 'between', 'the', 'and', 'japan', 'has', 'raised', 'fears', 'among', 'many', 'of', 'asia', 'exporting', 'nations', 'that', 'the', 'row', 'could', 'inflict', 'economic', 'damage', 'businessmen', 'and', 'officials', 'said', 'they', 'told', 'reuter', 'correspondents', 'in', 'asian', 'capitals', 'a', 'move', 'against', 'japan', 'might', 'boost', 'protectionist', 'sentiment', 'in', 'the', 'and', 'lead', 'to', 'curbs', 'on', 'american', 'imports', 'of', 'their', 'products', 'but', 'some', 'exporters', 'said', 'that', 'while', 'the', 'conflict', 'would', 'hurt', 'them', 'in', 'the', 'in', 'the', 'tokyo', 'loss', 'might', 'be', 'their', 'gain', 'the', 'has', 'said', 'it', 'will', 'impose', '300', 'mln', 'dlrs', 'of', 'tariffs', 'on', 'imports', 'of', 'japanese', 'electronics', 'goods', 'on', 'april', '17', 'in', 'retaliation', 'for', 'japan', 'alleged', 'failure', 'to', 'stick', 'to', 'a', 'pact', 'not', 'to', 'sell', 'semiconductors', 'on', 'world', 'markets', 'at', 'below', 'cost', 'unofficial', 'japanese', 'estimates', 'put', 'the', 'impact', 'of', 'the', 'tariffs', 'at', '10', 'billion', 'dlrs', 'and', 'spokesmen', 'for', 'major', 'electronics', 'firms', 'said', 'they', 'would', 'virtually', 'halt', 'exports', 'of', 'products', 'hit', 'by', 'the', 'new', 'taxes', 'we', 'would', 'be', 'able', 'to', 'do', 'business', 'said', 'a', 'spokesman', 'for', 'leading', 'japanese', 'electronics', 'firm', 'matsushita', 'electric', 'industrial', 'co', 'ltd', 'lt', 'if', 'the', 'tariffs', 'remain', 'in', 'place', 'for', 'any', 'length', 'of', 'time', 'beyond', 'a', 'few', 'months', 'it', 'will', 'mean', 'the', 'complete', 'erosion', 'of', 'exports', 'of', 'goods', 'subject', 'to', 'tariffs', 'to', 'the', 'said', 'tom', 'murtha', 'a', 'stock', 'analyst', 'at', 'the', 'tokyo', 'office', 'of', 'broker', 'lt', 'james', 'capel', 'and', 'co', 'in', 'taiwan', 'businessmen', 'and', 'officials', 'are', 'also', 'worried', 'we', 'are', 'aware', 'of', 'the', 'seriousness', 'of', 'the', 'threat', 'against', 'japan', 'because', 'it', 'serves', 'as', 'a', 'warning', 'to', 'us', 'said', 'a', 'senior', 'taiwanese', 'trade', 'official', 'who', 'asked', 'not', 'to', 'be', 'named', 'taiwan', 'had', 'a', 'trade', 'trade', 'surplus', 'of', 'billion', 'dlrs', 'last', 'year', '95', 'pct', 'of', 'it', 'with', 'the', 'the', 'surplus', 'helped', 'swell', 'taiwan', 'foreign', 'exchange', 'reserves', 'to', '53', 'billion', 'dlrs', 'among', 'the', 'world', 'largest', 'we', 'must', 'quickly', 'open', 'our', 'markets', 'remove', 'trade', 'barriers', 'and', 'cut', 'import', 'tariffs', 'to', 'allow', 'imports', 'of', 'products', 'if', 'we', 'want', 'to', 'defuse', 'problems', 'from', 'possible', 'retaliation', 'said', 'paul', 'sheen', 'chairman', 'of', 'textile', 'exporters', 'lt', 'taiwan', 'safe', 'group', 'a', 'senior', 'official', 'of', 'south', 'korea', 'trade', 'promotion', 'association', 'said', 'the', 'trade', 'dispute', 'between', 'the', 'and', 'japan', 'might', 'also', 'lead', 'to', 'pressure', 'on', 'south', 'korea', 'whose', 'chief', 'exports', 'are', 'similar', 'to', 'those', 'of', 'japan', 'last', 'year', 'south', 'korea', 'had', 'a', 'trade', 'surplus', 'of', 'billion', 'dlrs', 'with', 'the', 'up', 'from', 'billion', 'dlrs', 'in', 'in', 'malaysia', 'trade', 'officers', 'and', 'businessmen', 'said', 'tough', 'curbs', 'against', 'japan', 'might', 'allow', 'producers', 'of', 'semiconductors', 'in', 'third', 'countries', 'to', 'expand', 'their', 'sales', 'to', 'the', 'in', 'hong', 'kong', 'where', 'newspapers', 'have', 'alleged', 'japan', 'has', 'been', 'selling', 'semiconductors', 'some', 'electronics', 'manufacturers', 'share', 'that', 'view', 'but', 'other', 'businessmen', 'said', 'such', 'a', 'commercial', 'advantage', 'would', 'be', 'outweighed', 'by', 'further', 'pressure', 'to', 'block', 'imports', 'that', 'is', 'a', 'very', 'view', 'said', 'lawrence', 'mills', 'of', 'the', 'federation', 'of', 'hong', 'kong', 'industry', 'if', 'the', 'whole', 'purpose', 'is', 'to', 'prevent', 'imports', 'one', 'day', 'it', 'will', 'be', 'extended', 'to', 'other', 'sources', 'much', 'more', 'serious', 'for', 'hong', 'kong', 'is', 'the', 'disadvantage', 'of', 'action', 'restraining', 'trade', 'he', 'said', 'the', 'last', 'year', 'was', 'hong', 'kong', 'biggest', 'export', 'market', 'accounting', 'for', 'over', '30', 'pct', 'of', 'domestically', 'produced', 'exports', 'the', 'australian', 'government', 'is', 'awaiting', 'the', 'outcome', 'of', 'trade', 'talks', 'between', 'the', 'and', 'japan', 'with', 'interest', 'and', 'concern', 'industry', 'minister', 'john', 'button', 'said', 'in', 'canberra', 'last', 'friday', 'this', 'kind', 'of', 'deterioration', 'in', 'trade', 'relations', 'between', 'two', 'countries', 'which', 'are', 'major', 'trading', 'partners', 'of', 'ours', 'is', 'a', 'very', 'serious', 'matter', 'button', 'said', 'he', 'said', 'australia', 'concerns', 'centred', 'on', 'coal', 'and', 'beef', 'australia', 'two', 'largest', 'exports', 'to', 'japan', 'and', 'also', 'significant', 'exports', 'to', 'that', 'country', 'meanwhile', 'diplomatic', 'manoeuvres', 'to', 'solve', 'the', 'trade', 'continue', 'japan', 'ruling', 'liberal', 'democratic', 'party', 'yesterday', 'outlined', 'a', 'package', 'of', 'economic', 'measures', 'to', 'boost', 'the', 'japanese', 'economy', 'the', 'measures', 'proposed', 'include', 'a', 'large', 'supplementary', 'budget', 'and', 'record', 'public', 'works', 'spending', 'in', 'the', 'first', 'half', 'of', 'the', 'financial', 'year', 'they', 'also', 'call', 'for', 'spending', 'as', 'an', 'emergency', 'measure', 'to', 'stimulate', 'the', 'economy', 'despite', 'prime', 'minister', 'yasuhiro', 'nakasone', 'avowed', 'fiscal', 'reform', 'program', 'deputy', 'trade', 'representative', 'michael', 'smith', 'and', 'makoto', 'kuroda', 'japan', 'deputy', 'minister', 'of', 'international', 'trade', 'and', 'industry', 'miti', 'are', 'due', 'to', 'meet', 'in', 'washington', 'this', 'week', 'in', 'an', 'effort', 'to', 'end', 'the', 'dispute']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import reuters\n",
    "import string\n",
    "\n",
    "# Download dataset and tokenizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('reuters')\n",
    "\n",
    "# Load Reuters corpus (sample dataset)\n",
    "sentences = [word_tokenize(reuters.raw(file_id).lower()) for file_id in reuters.fileids()]\n",
    "\n",
    "# Remove punctuation\n",
    "sentences = [[word for word in sentence if word.isalnum()] for sentence in sentences]\n",
    "\n",
    "print(f\"Sample sentence: {sentences[0]}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-10T18:12:04.992775Z",
     "start_time": "2025-03-10T18:11:45.981309Z"
    }
   },
   "id": "ba15bb839f28a5d9",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Train Word2Vec model\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=2, workers=4)\n",
    "\n",
    "# Save model\n",
    "model.save(\"word2vec_reuters.model\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-10T18:22:40.009019Z",
     "start_time": "2025-03-10T18:22:38.325661Z"
    }
   },
   "id": "536e5624084dea46",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 'rope' not in vocabulary.\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "model = Word2Vec.load(\"word2vec_reuters.model\")\n",
    "\n",
    "# Find similar words\n",
    "word = \"rope\"\n",
    "if word in model.wv:\n",
    "    similar_words = model.wv.most_similar(word, topn=5)\n",
    "    print(f\"Words similar to '{word}': {similar_words}\")\n",
    "else:\n",
    "    print(f\"Word '{word}' not in vocabulary.\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-10T18:33:11.733154Z",
     "start_time": "2025-03-10T18:33:11.659678Z"
    }
   },
   "id": "8a970863cffb50bf",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Co-occuence matrix "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be1078abfb2560c6"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 27.0004\n",
      "Epoch 500, Loss: 0.0037\n",
      "Epoch 1000, Loss: 0.0004\n",
      "Epoch 1500, Loss: 0.0003\n",
      "Epoch 2000, Loss: 0.0003\n",
      "Epoch 2500, Loss: 0.0003\n",
      "Epoch 3000, Loss: 0.0002\n",
      "Epoch 3500, Loss: 0.0002\n",
      "Epoch 4000, Loss: 0.0002\n",
      "Epoch 4500, Loss: 0.0002\n",
      "\n",
      "Word Embeddings:\n",
      "Word 0: [0.9650556424386794, 0.1919270311002789, -0.14527167982458658]\n",
      "Word 1: [0.958565067577809, 0.23804208131819674, -0.17277550622386595]\n",
      "Word 2: [0.9606294327740452, 0.2535995705445823, -0.11155500357093034]\n",
      "Word 3: [0.9171117069091419, 0.3547063078973181, -0.17876467230158663]\n",
      "Word 4: [0.8772820597188794, 0.46108695391963056, -0.12723690801378695]\n",
      "Word 5: [0.8762592551041872, 0.4620669448888442, -0.131150757772208]\n",
      "Word 6: [0.8577030222680163, 0.526559685028864, -0.002359915552266447]\n",
      "Word 7: [0.8391726677668584, 0.555511207741579, -0.052391152516393334]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def initialize_matrices(vocab_size, embedding_dim):\n",
    "    # Initialize word and context matrices with small random values\n",
    "    W = [[random.uniform(-0.01, 0.01) for _ in range(embedding_dim)] for _ in range(vocab_size)]\n",
    "    C = [[random.uniform(-0.01, 0.01) for _ in range(embedding_dim)] for _ in range(vocab_size)]\n",
    "    return W, C\n",
    "\n",
    "def matrix_factorization(M, W, C, learning_rate=0.01, epochs=5000):\n",
    "    vocab_size = len(M)\n",
    "    embedding_dim = len(W[0])\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for i in range(vocab_size):\n",
    "            for j in range(vocab_size):\n",
    "                if M[i][j] > 0:  # Only update for non-zero co-occurrence\n",
    "                    # Compute prediction\n",
    "                    predicted = sum(W[i][k] * C[j][k] for k in range(embedding_dim))\n",
    "                    error = M[i][j] - predicted\n",
    "\n",
    "                    # Update word and context vectors using gradient descent\n",
    "                    for k in range(embedding_dim):\n",
    "                        W[i][k] += learning_rate * error * C[j][k]\n",
    "                        C[j][k] += learning_rate * error * W[i][k]\n",
    "\n",
    "                    total_loss += error ** 2  # Compute loss\n",
    "\n",
    "        if epoch % 500 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "    return W, C\n",
    "\n",
    "# Example Co-occurrence Matrix (from previous step)\n",
    "M = [\n",
    "    [0, 1, 0, 0, 0, 0, 0, 0],  # \"I\"\n",
    "    [1, 0, 1, 1, 0, 0, 0, 0],  # \"want\"\n",
    "    [1, 1, 0, 1, 1, 0, 0, 0],                # to\n",
    "    [0, 1, 1, 0, 1, 1 ,0, 0],                # get\n",
    "    [0, 0, 1, 1, 0, 1, 1, 0],                # good\n",
    "    [0, 0, 1, 1, 0, 1, 1, 0],                # at\n",
    "    [0, 0, 0, 1, 1, 0, 1, 1],  # \"deep\"\n",
    "    [0, 0, 0, 0, 1, 1, 0, 1]   # \"learning\"\n",
    "]\n",
    "\n",
    "# Define parameters\n",
    "vocab_size = len(M)\n",
    "embedding_dim = 3  # Reduce to 2D for visualization\n",
    "\n",
    "# Initialize matrices\n",
    "W, C = initialize_matrices(vocab_size, embedding_dim)\n",
    "\n",
    "# Train embeddings using matrix factorization\n",
    "word_embeddings, context_embeddings = matrix_factorization(M, W, C)\n",
    "\n",
    "print(\"\\nWord Embeddings:\")\n",
    "for i, vec in enumerate(word_embeddings):\n",
    "    print(f\"Word {i}: {vec}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-11T05:34:43.218055Z",
     "start_time": "2025-03-11T05:34:43.013954Z"
    }
   },
   "id": "e46a56581e6d43b",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Skip gram"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d897b5306409c356"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample training pairs (word indices): [(7, 1), (7, 4), (1, 7), (1, 4), (1, 3), (4, 7), (4, 1), (4, 3), (4, 5), (3, 1), (3, 4), (3, 5), (3, 0), (5, 4), (5, 3), (5, 0), (5, 2), (0, 3), (0, 5), (0, 2), (0, 6), (2, 5), (2, 0), (2, 6), (6, 0), (6, 2)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Sample text corpus\n",
    "corpus = \"I love deep learning and natural language processing\"\n",
    "\n",
    "# Tokenize (split into words)\n",
    "words = corpus.split()\n",
    "vocab = list(set(words))  # Unique words\n",
    "word_to_id = {word: i for i, word in enumerate(vocab)}  # Map word → index\n",
    "id_to_word = {i: word for word, i in word_to_id.items()}  # Map index → word\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Parameters\n",
    "window_size = 2  # How many words on each side we consider as context\n",
    "\n",
    "# Generate training pairs (target, context)\n",
    "training_pairs = []\n",
    "\n",
    "for i, target in enumerate(words):\n",
    "    target_idx = word_to_id[target]\n",
    "    start = max(0, i - window_size)\n",
    "    end = min(len(words), i + window_size + 1)\n",
    "    \n",
    "    for j in range(start, end):\n",
    "        if i != j:  # Avoid pairing the word with itself\n",
    "            context_idx = word_to_id[words[j]]\n",
    "            training_pairs.append((target_idx, context_idx))\n",
    "\n",
    "print(\"Sample training pairs (word indices):\", training_pairs)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-12T05:08:53.995372Z",
     "start_time": "2025-03-12T05:08:53.977660Z"
    }
   },
   "id": "842d5f930565b501",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Embedding size (dimension of word vectors)\n",
    "embedding_dim = 5  \n",
    "\n",
    "# Initialize weight matrices randomly\n",
    "W = np.random.randn(vocab_size, embedding_dim) * 0.01\n",
    "C = np.random.randn(embedding_dim, vocab_size) * 0.01\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-12T05:09:57.888906Z",
     "start_time": "2025-03-12T05:09:57.875093Z"
    }
   },
   "id": "88535a37a2fe8fde",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))  # Numerical stability\n",
    "    return e_x / np.sum(e_x)\n",
    "\n",
    "def forward_pass(target_idx):\n",
    "    # Lookup word embedding\n",
    "    v_target = W[target_idx]  # Shape: (embedding_dim,)\n",
    "    \n",
    "    # Compute scores (dot product with context matrix)\n",
    "    scores = np.dot(v_target, C)  # Shape: (vocab_size,)\n",
    "    \n",
    "    # Convert to probability distribution using softmax\n",
    "    y_pred = softmax(scores)  # Shape: (vocab_size,)\n",
    "    \n",
    "    return y_pred  # Probability distribution over all words\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-12T05:10:12.984085Z",
     "start_time": "2025-03-12T05:10:12.978806Z"
    }
   },
   "id": "74b32ebdf6abe924",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def compute_loss(y_pred, true_idx):\n",
    "    return -np.log(y_pred[true_idx])  # Negative log likelihood\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-12T05:13:34.653915Z",
     "start_time": "2025-03-12T05:13:34.644592Z"
    }
   },
   "id": "1d73758ce8dcef2",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def backward_pass(target_idx, context_idx, y_pred, C, learning_rate=0.01):\n",
    "    # One-hot encoding for the true context word\n",
    "    y_true = np.zeros(vocab_size)\n",
    "    y_true[context_idx] = 1  # The actual context word\n",
    "\n",
    "    # Compute error\n",
    "    error = y_pred - y_true  # Shape: (vocab_size,)\n",
    "\n",
    "    # Compute gradients\n",
    "    dW = np.outer(error, C.T)  # Gradient for W\n",
    "    dC = np.outer(W[target_idx], error)  # Gradient for C\n",
    "\n",
    "    # Update parameters\n",
    "    W[target_idx] -= learning_rate * dW.sum(axis=0)\n",
    "    C -= learning_rate * dC\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-12T05:41:28.426682Z",
     "start_time": "2025-03-12T05:41:28.418953Z"
    }
   },
   "id": "c1500f0d071434e1",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (5,) (40,) (5,) ",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[17], line 15\u001B[0m\n\u001B[1;32m     12\u001B[0m     loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m compute_loss(y_pred, context_idx)\n\u001B[1;32m     14\u001B[0m     \u001B[38;5;66;03m# Backward pass (update weights)\u001B[39;00m\n\u001B[0;32m---> 15\u001B[0m     \u001B[43mbackward_pass\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtarget_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcontext_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_pred\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mC\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m epoch \u001B[38;5;241m%\u001B[39m \u001B[38;5;241m100\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m     18\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mloss\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn[15], line 14\u001B[0m, in \u001B[0;36mbackward_pass\u001B[0;34m(target_idx, context_idx, y_pred, C, learning_rate)\u001B[0m\n\u001B[1;32m     11\u001B[0m dC \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mouter(W[target_idx], error)  \u001B[38;5;66;03m# Gradient for C\u001B[39;00m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# Update parameters\u001B[39;00m\n\u001B[0;32m---> 14\u001B[0m W[target_idx] \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m learning_rate \u001B[38;5;241m*\u001B[39m dW\u001B[38;5;241m.\u001B[39msum(axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m     15\u001B[0m C \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m learning_rate \u001B[38;5;241m*\u001B[39m dC\n",
      "\u001B[0;31mValueError\u001B[0m: operands could not be broadcast together with shapes (5,) (40,) (5,) "
     ]
    }
   ],
   "source": [
    "# Training parameters\n",
    "epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loss = 0\n",
    "    for target_idx, context_idx in training_pairs:\n",
    "        # Forward pass\n",
    "        y_pred = forward_pass(target_idx)\n",
    "\n",
    "        # Compute loss\n",
    "        loss += compute_loss(y_pred, context_idx)\n",
    "\n",
    "        # Backward pass (update weights)\n",
    "        backward_pass(target_idx, context_idx, y_pred, C, learning_rate)\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-12T05:41:43.823394Z",
     "start_time": "2025-03-12T05:41:43.799732Z"
    }
   },
   "id": "a9300d3a713fcc44",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1a4d130b8dc5271f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
